{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d9ab20-42ef-4466-8da6-fa483c7dd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "config = ['Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Computer_Science', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Energy_and_Power', 'Finance', 'Geography', 'History', 'Literature', 'Manage', 'Marketing', 'Materials', 'Math', 'Mechanical_Engineering', 'Music', 'Pharmacy', 'Physics', 'Psychology', 'Public_Health', 'Sociology']\n",
    "\n",
    "ds = load_dataset(\"MMMU/MMMU\",config[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d843f56d-b76f-4544-a41e-2359d8761f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5a92c0-6aa1-40ff-85ba-a02259400c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b5825055af4951bdae802312ebfb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlavaNextForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to(f\"cuda:{gpu_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7514907f-fbff-4e63-884d-1ed4ee0ccc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f5418166050>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2ee0c6-9f47-47df-9abb-af9c40d95ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29ec1aa-6a72-4ad6-8b82-9802bb9e84f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/vision/torralba/selfmanaged/torralba/scratch/schwett/meow/FINAL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c106e575-6937-4a66-8f5c-062138af31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 8\n",
    "file = f\"/data/vision/torralba/selfmanaged/torralba/scratch/schwett/meow/SAE_imagetext_layer{layer}_models_longrun/\" +os.listdir(f\"/data/vision/torralba/selfmanaged/torralba/scratch/schwett/meow/SAE_imagetext_layer{layer}_models_longrun/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe12903-3adb-404f-9a08-543bae33389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3485483/4241840292.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae.load_state_dict(torch.load(file))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096,int(8*4096),bias=True) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(int(8*4096), 4096,bias=True) \n",
    "        self.initialize_weights()\n",
    "    #Biases are initialized to zero, \n",
    "    def initialize_weights(self):\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        #fun trick, gaussians in high dimensions are uniform \"soap bubbles\"\n",
    "        W_d = torch.randn(self.fc2.weight.size())\n",
    "        W_d = W_d / W_d.norm(dim=1, keepdim=True) * 0.1  # Adjust 0.1 as needed\n",
    "        self.fc2.weight.data = W_d\n",
    "        self.fc1.weight.data = W_d.t()\n",
    "    def forward(self, x):\n",
    "        features = self.relu1(self.fc1(x))\n",
    "        x_hat = self.fc2(features)\n",
    "        return x_hat,features \n",
    "sae = SAE()\n",
    "sae = nn.DataParallel(sae, device_ids=[gpu_index])\n",
    "sae = sae.cuda(device=gpu_index)\n",
    "sae.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6f4343-e301-459e-8483-f177ed37dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ablate(text_input,image,tokens):\n",
    "        intervention = intervene\n",
    "        inp = processor(text_input,image, return_tensors=\"pt\").to(training_gpu)\n",
    "        for i in tqdm(range(tokens)):    \n",
    "            with Trace(model.language_model.model.layers[layer-1],edit_output=intervention) as ret:\n",
    "                out = model(**inp)\n",
    "            logits = out.logits[0][-1]\n",
    "            inp['input_ids'] = torch.tensor(np.array(list(inp.input_ids.detach().cpu().numpy()[0])+[torch.argmax(logits).detach().cpu().numpy()])).unsqueeze(0).to(training_gpu)\n",
    "            inp['attention_mask'] = torch.ones((inp['attention_mask'].size(0),inp['attention_mask'].size(1)+1)).int().to(training_gpu)\n",
    "        return inp['input_ids'][0]\n",
    "    \n",
    "def generate_ablate(input_text,imp_image,num_tokens):\n",
    "    ids = forward_ablate(input_text,num_tokens)\n",
    "    return processor.decode(ids,skip_special_tokens=True)\n",
    "\n",
    "def intervene(output):\n",
    "    meow = output[0].squeeze(0).float()\n",
    "    meow = sae(meow.unsqueeze(0)*8).squeeze(0)/8\n",
    "    return (meow.unsqueeze(0).half(),output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "665e4afc-ce32-4604-93fe-c60d280545d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1985f986-1220-4fca-a24f-70bbdd29b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_q(i):\n",
    "    q = '[INST] '+ds[i]['question'].replace('<image 1>','\\n<image>\\n').replace('<image 2>','<image>').replace('<image 3>','<image>').replace('<image 4>','<image>').replace('<image 5>','<image>').replace('<image 6>','<image>').replace('<image 7>','<image>')\n",
    "    options = ast.literal_eval(ds[i]['options'])\n",
    "    letters = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N']\n",
    "    used_letters = []\n",
    "    for j in range(len(options)):\n",
    "        q = q+f\"\\n {letters[j]}: \"+options[j]\n",
    "        used_letters.append(letters[j])\n",
    "    \n",
    "    q = q + f\"\\n Please Answer only answer with one of {used_letters}. Do not provide any explanation. [/INST]\"\n",
    "    images = []\n",
    "    for j in range(1,8):\n",
    "        if ds[i]['image_'+str(j)] is not None:\n",
    "            images.append(ds[i]['image_'+str(j)])\n",
    "    return q,images, ds[i]['answer'], len(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06dc6b36-f893-439c-8ff1-191abe3c9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "# Turn off warnings from the transformers library\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# If you want to suppress all Python warnings, including those from other libraries\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94052829-c1f0-47c3-88fd-ab6f06edef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6825d16-91f9-4b57-a419-f77e3ad91f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msadge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[43mcnt\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtot\u001b[49m, \u001b[38;5;241m4\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Chance: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(chance\u001b[38;5;241m/\u001b[39mtot, \u001b[38;5;241m4\u001b[39m)))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(cnt\u001b[38;5;241m/\u001b[39mtot)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "cnt = 0\n",
    "tot = 0\n",
    "chance = 0\n",
    "\n",
    "for l in range(len(config)):\n",
    "    ds = load_dataset(\"MMMU/MMMU\",config[l])\n",
    "    ds = ds['validation']\n",
    "    pbar = tqdm(range(30))\n",
    "    for i in pbar:\n",
    "        prompt, images, answer,num_answers = load_q(i)\n",
    "        try:\n",
    "            inputs = processor(images, prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "            \n",
    "            # autoregressively complete prompt\n",
    "            output = generate_ablate(prompt,image,1)\n",
    "        \n",
    "            rep = processor.decode(output[0], skip_special_tokens=True)[-1]\n",
    "            if rep == answer:\n",
    "                cnt+=1\n",
    "            if rep not in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N']:\n",
    "                print(\"error\")\n",
    "                output = model.generate(**inputs, pad_token_id=model.config.eos_token_id,max_new_tokens=30)\n",
    "                print(processor.decode(output[0], skip_special_tokens=True))\n",
    "                print(processor.decode(output[0], skip_special_tokens=True)[-1])\n",
    "                \n",
    "            tot +=1\n",
    "            chance+=1/num_answers\n",
    "            pbar.set_description(\"MODEL: \"+str(round(cnt/tot, 4)) + \" Chance: \"+ str(round(chance/tot, 4)))\n",
    "        except:\n",
    "            print(\"sadge\")\n",
    "        \n",
    "print(cnt/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "909b8598-32f5-4094-b6bd-4dec23f35b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.pad_token_id=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c54b33-a4d1-4ffc-a80f-045c61018cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "556e9996-6645-443b-8500-8ea5bf40ae63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visual elements'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07da2dd-6edf-45f0-89d3-ce34886498a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Which of the following best explains the overall trend shown in the <image>?\\n A: [\\n B: '\\n C: M\\n D: M\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9483dcef-2c18-4295-aad9-fdf59943623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff190ed29f3f4a6ea681f4a9ede59c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlavaNextForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df742882-7b3a-4ba3-894b-952bc028c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (3.0.3)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2) (2.0.1)\n",
      "Installing collected packages: jinja2\n",
      "Successfully installed jinja2-3.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9a9d2-445b-4184-b3c2-c360e2db86bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
